{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0N0cfV6CV0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f4db6b53-efd8-490f-a1a2-b5cbd17404e8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import scipy\n",
        "import sklearn\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import libary for TFID Vectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from  sklearn.metrics  import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Import feature selection Libraries\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression, mutual_info_classif\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb__9tSGCbcW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "f7bf0f34-53fd-4d48-a3a3-6a51ff247130"
      },
      "source": [
        "!wget -O dataset_combined_uncleaned.csv https://raw.githubusercontent.com/godhi06/TA/master/data/dataset_combined_uncleaned.csv\n",
        "!wget -O df_pre1.csv https://raw.githubusercontent.com/godhi06/TA/master/data/preprocessing/dataset_preprocessed_1.csv\n",
        "\n",
        "\n",
        "df_ori = pd.read_csv('dataset_combined_uncleaned.csv', index_col=0)\n",
        "df_pre1 = pd.read_csv('df_pre1.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-03 03:01:13--  https://raw.githubusercontent.com/godhi06/TA/master/data/dataset_combined_uncleaned.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 223725 (218K) [text/plain]\n",
            "Saving to: ‘dataset_combined_uncleaned.csv’\n",
            "\n",
            "\r          dataset_c   0%[                    ]       0  --.-KB/s               \rdataset_combined_un 100%[===================>] 218.48K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-08-03 03:01:14 (4.25 MB/s) - ‘dataset_combined_uncleaned.csv’ saved [223725/223725]\n",
            "\n",
            "--2020-08-03 03:01:18--  https://raw.githubusercontent.com/godhi06/TA/master/data/preprocessing/dataset_preprocessed_1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 215246 (210K) [text/plain]\n",
            "Saving to: ‘df_pre1.csv’\n",
            "\n",
            "df_pre1.csv         100%[===================>] 210.20K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-08-03 03:01:18 (3.98 MB/s) - ‘df_pre1.csv’ saved [215246/215246]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA_MyyfYRA7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6734420f-7fa5-40e6-817e-7c4496d9dc18"
      },
      "source": [
        "df_ori_dep = df_ori[df_ori.label == 1]\n",
        "df_ori_nonDep = df_ori[df_ori.label == 0]\n",
        "print(\"Banyak data original:\", len(df_ori))\n",
        "print(\"Banyak data depresi:\", len(df_ori_dep))\n",
        "print(\"Banyak data non-depresi:\", len(df_ori_nonDep))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Banyak data original: 657\n",
            "Banyak data depresi: 340\n",
            "Banyak data non-depresi: 317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciAsJBgvRBKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FvmgB2PDVBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ori = df_ori.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxHQWDgmDYMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "977b1b9c-daed-47ab-afe5-4fb112ff5907"
      },
      "source": [
        "df_ori_dep = df_ori[df_ori.label == 1]\n",
        "df_ori_nonDep = df_ori[df_ori.label == 0]\n",
        "print(\"Banyak data original:\", len(df_ori))\n",
        "print(\"Banyak data depresi:\", len(df_ori_dep))\n",
        "print(\"Banyak data non-depresi:\", len(df_ori_nonDep))\n",
        "print('\\n')\n",
        "print(\"Banyak data depresi dfPre1:\", len(df_pre1[df_pre1.label == 1]))\n",
        "print(\"Banyak data non-depresi dfPre1:\", len(df_pre1[df_pre1.label == 0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Banyak data original: 651\n",
            "Banyak data depresi: 340\n",
            "Banyak data non-depresi: 311\n",
            "\n",
            "\n",
            "Banyak data depresi dfPre1: 340\n",
            "Banyak data non-depresi dfPre1: 307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY3xXlCQFFqI",
        "colab_type": "text"
      },
      "source": [
        "PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSuOI8uMDfLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_lowerCase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def expand_contractions(data):\n",
        "    data = re.sub(r\"\\bdon't\\b\", 'do not', data)\n",
        "    data = re.sub(r\"\\bdidn't\\b\", 'did not', data)\n",
        "    data = re.sub(r\"\\bdoesn't\\b\", 'does not', data)\n",
        "    data = re.sub(r\"\\bisn't\\b\", 'is not', data)\n",
        "    data = re.sub(r\"\\baren't\\b\", 'are not', data)\n",
        "    data = re.sub(r\"\\bwasn't\\b\", 'was not', data)\n",
        "    data = re.sub(r\"\\bweren't\\b\", 'were not', data)\n",
        "\n",
        "    data = re.sub(r\"\\bhadn't\\b\", 'had not', data)\n",
        "    data = re.sub(r\"\\bhadn't've\\b\", 'had not have', data)\n",
        "    data = re.sub(r\"\\bhasn't\\b\", 'has not', data)\n",
        "    data = re.sub(r\"\\bhaven't\\b\", 'have not', data)\n",
        "\n",
        "    data = re.sub(r\"\\bcan't\\b\", 'can not', data)\n",
        "    data = re.sub(r\"\\bcan't've\\b\", 'cannot have', data)\n",
        "    data = re.sub(r\"\\bcould've\\b\", 'could have', data)\n",
        "    data = re.sub(r\"\\bcouldn't\\b\", 'could not', data)\n",
        "    data = re.sub(r\"\\bcouldn't've\\b\", 'could not have', data)\n",
        "    data = re.sub(r\"\\bshould've\\b\", 'should have', data)\n",
        "    data = re.sub(r\"\\bshouldn't\\b\", 'should not', data)\n",
        "    data = re.sub(r\"\\bshouldn't've\\b\", 'should not have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bi'll\\b\", 'i will', data)\n",
        "    data = re.sub(r\"\\bi'll've\\b\", 'i will have', data)\n",
        "    data = re.sub(r\"\\bi'm\\b\", 'i am', data)\n",
        "    data = re.sub(r\"\\bi've\\b\", 'i have', data)\n",
        "    data = re.sub(r\"\\bi'd\\b\", 'i would', data)\n",
        "    data = re.sub(r\"\\bi'd've\\b\", 'i would have', data)\n",
        "\n",
        "    data = re.sub(r\"\\by'all\\b\", 'you all', data)\n",
        "    data = re.sub(r\"\\by'all're\\b\", 'you all are', data)\n",
        "    data = re.sub(r\"\\byou're\\b\", 'you are', data)\n",
        "    data = re.sub(r\"\\byou've\\b\", 'you have', data)\n",
        "    data = re.sub(r\"\\byou'll\\b\", 'you will', data)\n",
        "    data = re.sub(r\"\\byou'll've\\b\", 'you will have', data)\n",
        "    data = re.sub(r\"\\byou'd\\b\", 'you would', data)\n",
        "    data = re.sub(r\"\\byou'd've\\b\", 'you would have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bwe're\\b\", 'we re', data)\n",
        "    data = re.sub(r\"\\bwe've\\b\", 'we have', data)\n",
        "    data = re.sub(r\"\\bwe'll\\b\", 'we will', data)\n",
        "    data = re.sub(r\"\\bwe'll've\\b\", 'we will have', data)\n",
        "    data = re.sub(r\"\\bwe'd\\b\", 'we would', data)\n",
        "    data = re.sub(r\"\\bwe'd've\\b\", 'we would have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bthey're\\b\", 'they are', data)\n",
        "    data = re.sub(r\"\\bthey've\\b\", 'they have', data)\n",
        "    data = re.sub(r\"\\bthey'll\\b\", 'they will', data)\n",
        "    data = re.sub(r\"\\bthey'll've\\b\", 'they will have', data)\n",
        "    data = re.sub(r\"\\bthey'd\\b\", 'they would', data)\n",
        "    data = re.sub(r\"\\bthey'd've\\b\", 'they would have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bhe's\\b\", 'he is', data)\n",
        "    data = re.sub(r\"\\bhe'd\\b\", 'he would', data)\n",
        "    data = re.sub(r\"\\bhe'd've\\b\", 'he would have', data)\n",
        "    data = re.sub(r\"\\bhe'll\\b\", 'he will', data)\n",
        "    data = re.sub(r\"\\bhe'll've\\b\", 'he will have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bshe's\\b\", 'she is', data)\n",
        "    data = re.sub(r\"\\bshe'd\\b\", 'she would', data)\n",
        "    data = re.sub(r\"\\bshe'd've\\b\", 'she would have', data)\n",
        "    data = re.sub(r\"\\bshe'll\\b\", 'she will', data)\n",
        "    data = re.sub(r\"\\bshe'll've\\b\", 'she will have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bit's\\b\", 'it is', data)\n",
        "    data = re.sub(r\"\\bit'd\\b\", 'it would', data)\n",
        "    data = re.sub(r\"\\bit'd've\\b\", 'it would have', data)\n",
        "    data = re.sub(r\"\\bit'll\\b\", 'it will', data)\n",
        "\n",
        "    data = re.sub(r\"\\bthat's\\b\", 'that is', data)\n",
        "    data = re.sub(r\"\\bthat'd\\b\", 'that would', data)\n",
        "    data = re.sub(r\"\\bthat'd've\\b\", 'that would have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bthere's\\b\", 'there is', data)\n",
        "    data = re.sub(r\"\\bthere'd\\b\", 'there would', data)\n",
        "    data = re.sub(r\"\\bthere'd've\\b\", 'there would have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bwhat's\\b\", 'what is', data)\n",
        "    data = re.sub(r\"\\bwhat're\\b\", 'what are', data)\n",
        "    data = re.sub(r\"\\bwhat'd\\b\", 'what would', data)\n",
        "    data = re.sub(r\"\\bwhat've\\b\", 'what have', data)\n",
        "\n",
        "    data = re.sub(r\"\\bwhat's\\b\", 'when is', data)\n",
        "    data = re.sub(r\"\\bwhat're\\b\", 'where is', data)\n",
        "    data = re.sub(r\"\\bwhat'd\\b\", 'who is', data)\n",
        "    data = re.sub(r\"\\bwhat've\\b\", 'who will', data)\n",
        "\n",
        "    data = re.sub(r\"\\bwill've\\b\", 'will have', data)\n",
        "    data = re.sub(r\"\\bwon't\\b\", 'will not', data)\n",
        "    data = re.sub(r\"\\bwould've\\b\", 'would have', data)\n",
        "    data = re.sub(r\"\\bwouldn't\\b\", 'would not', data)\n",
        "\n",
        "    data = re.sub(r\"\\bhow'd\\b\", 'how did', data)\n",
        "    data = re.sub(r\"\\bhow'd'yb\", 'how do you', data)\n",
        "    data = re.sub(r\"\\bhow'll\\b\", 'how will', data)\n",
        "    data = re.sub(r\"\\bhow's\\b\", 'how is', data)\n",
        "\n",
        "    data = re.sub(r\"\\bmight've\\b\", 'might have', data)\n",
        "    data = re.sub(r\"\\bmightn't\\b\", 'might not', data)\n",
        "    data = re.sub(r\"\\bmust've\\b\", 'must have', data)\n",
        "    data = re.sub(r\"\\bmustn't\\b\", 'must not', data)\n",
        "    data = re.sub(r\"\\bneedn't\\b\", 'need not', data)\n",
        "\n",
        "    data = re.sub(r\"\\b'cause\\b\", 'because', data)\n",
        "    data = re.sub(r\"\\blet's\\b\", 'let us', data)\n",
        "    data = re.sub(r\"\\bo'clock\\b\", 'of the clock', data)\n",
        "\n",
        "    data = data.split()\n",
        "    data = \" \".join(data)\n",
        "    return data\n",
        "\n",
        "# Remove URLs\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "    text = re.sub('\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "# Remove special characters\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Remove extra whitespaces\n",
        "def remove_extraspaces(text):\n",
        "    return re.sub(' +', ' ', text)\n",
        "\n",
        "def tokenized_tweet(text):\n",
        "    return text.split()\n",
        "\n",
        "def cleaning(text):\n",
        "    text = text_to_lowerCase(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_urls(text)\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_extraspaces(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVq_l2ssNHHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean data using cleaning function\n",
        "df_ori_pre1 = df_ori.copy()\n",
        "df_ori_pre1['text'] = df_ori.text.apply(cleaning)\n",
        "\n",
        "# drop duplicate\n",
        "df_ori = df_ori.drop_duplicates()\n",
        "df_ori_pre1 = df_ori_pre1.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBbE2HS2NJAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy dataframe untuk buat dataframe tanpa stopwords\n",
        "df_ori_pre2 = df_ori_pre1.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1KSUgvsNUzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ori_pre2_tok = df_ori_pre2.text.apply(lambda x: x.split())\n",
        "df_ori_pre2_tok = df_ori_pre2_tok.apply(lambda x: [w for w in x if w not in list(stop_words)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erN9W8cNNWq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ori_pre2['text'] = list(df_ori_pre2_tok.apply(lambda x: ' '.join(x)))\n",
        "df_ori_pre2['wc_count'] = df_ori_pre2.text.apply(lambda x: len(str(x).split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqMkhyuQNafs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_all = df_ori_pre2[df_ori_pre2.wc_count >= 3].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fve3M--Neis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sesuaikan semua dataFrame dengan idx_all \n",
        "df_ori = df_ori.loc[idx_all]\n",
        "df_ori_pre1 = df_ori_pre1.loc[idx_all]\n",
        "df_ori_pre2 = df_ori_pre2.loc[idx_all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVBivFDNNhX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "810c8828-e77e-4a2c-95e4-9fa8cce788e9"
      },
      "source": [
        "len(df_ori) == len(df_ori_pre1) == len(df_ori_pre2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbPBRAzxNjEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ori.reset_index(drop=True, inplace=True)\n",
        "df_ori_pre1.reset_index(drop=True, inplace=True)\n",
        "df_ori_pre2.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3SQt94xNnN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files #library untuk export dari googlecolab\n",
        "\n",
        "df_ori.to_csv('df_ori.csv')\n",
        "\n",
        "df_ori_pre1.to_csv('df_ori_pre1.csv')\n",
        "\n",
        "df_ori_pre2.to_csv('df_ori_pre2.csv')\n",
        "\n",
        "files.download('df_ori.csv')\n",
        "files.download('df_ori_pre1.csv')\n",
        "files.download('df_ori_pre2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgcUJsr2O5bL",
        "colab_type": "text"
      },
      "source": [
        "create data with stemming and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVLq74YxOaAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXeRDgGgO-VY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem_lemm(df):\n",
        "    pre_tok = df.text.apply(lambda x: x.split())\n",
        "    pre_tok_stem = pre_tok.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "    pre_tok_lemm = pre_tok.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
        "    \n",
        "    # Join tokens into dataframe\n",
        "    df_pre_stem = pd.DataFrame(pre_tok_stem.apply(lambda x: ' '.join(x)))\n",
        "    df_pre_lemm = pd.DataFrame(pre_tok_lemm.apply(lambda x: ' '.join(x)))\n",
        "\n",
        "    # Add labels into each dataframe\n",
        "    df_pre_stem['label'] = list(df.label)\n",
        "    df_pre_lemm['label'] = list(df.label)\n",
        "    \n",
        "    return [df_pre_stem, df_pre_lemm]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH06saTxPBOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre1_group = stem_lemm(df_ori_pre1)\n",
        "pre2_group = stem_lemm(df_ori_pre2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7TKnIR2PVtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files #library untuk export dari googlecolab\n",
        "\n",
        "pre1_group[0].to_csv('pre1_stemmed.csv')\n",
        "\n",
        "pre1_group[1].to_csv('pre1_lemma.csv')\n",
        "\n",
        "pre2_group[0].to_csv('pre2_stemmed.csv')\n",
        "\n",
        "pre2_group[1].to_csv('pre2_lemma.csv')\n",
        "\n",
        "files.download('pre1_stemmed.csv')\n",
        "files.download('pre1_lemma.csv')\n",
        "files.download('pre2_stemmed.csv')\n",
        "files.download('pre2_lemma.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqyhqWoYPP96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}