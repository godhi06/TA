{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cleaning_explor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCnEWsuGwPDp",
        "colab_type": "text"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUuw4wn9tlvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk \n",
        "import string\n",
        "import csv\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVvn1lYfwS7k",
        "colab_type": "text"
      },
      "source": [
        "Import dataset from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuhFfCEDtw6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "d10268eb-0197-4677-8477-57e6d16a9bc4"
      },
      "source": [
        "!wget -O dataset_combined_uncleaned.csv https://raw.githubusercontent.com/godhi06/TA/master/data/dataset_combined_uncleaned.csv\n",
        "\n",
        "df = pd.read_csv('dataset_combined_uncleaned.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 17:33:23--  https://raw.githubusercontent.com/godhi06/TA/master/data/dataset_combined_uncleaned.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 223725 (218K) [text/plain]\n",
            "Saving to: ‘dataset_combined_uncleaned.csv’\n",
            "\n",
            "\r          dataset_c   0%[                    ]       0  --.-KB/s               \rdataset_combined_un 100%[===================>] 218.48K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-06-19 17:33:23 (5.18 MB/s) - ‘dataset_combined_uncleaned.csv’ saved [223725/223725]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOh2Av-s1q7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bbb3cc9-4f42-43ba-cfbe-680e3f1a0820"
      },
      "source": [
        "print(\"Banyak data pada dataset combined uncleaned\", len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Banyak data pada dataset combined uncleaned 657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiILrhAiCWeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "26f09657-c9b7-4b02-97fa-0a91c5bdcd08"
      },
      "source": [
        "print(\"Berikut dataset dengan NA_values \\n\")\n",
        "df[df['text'].isnull() == True]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berikut dataset dengan NA_values \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, label]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF3RHKDDCbnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Buang data dengan NA_Values\n",
        "df = df[df['text'].isnull() == False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YwhNMfvqiIl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZaY1lIaCfva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "1035573d-817f-4d87-d313-e87d6e048509"
      },
      "source": [
        "# Menampilkan dataset tanpa duplicate rows\n",
        "print(\"Berikut dataset tanpa duplicate row \\n\")\n",
        "df.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berikut dataset tanpa duplicate row \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've had major depressive disorder for many y...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One of my friends lost his brother to suicide...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My long distance boyfriend sent me flowers for...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I was recently diagnosed with GAD (Generalize...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After 4 years my hair is finally healed, &amp; lif...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>I wish I could shake this. I don't laugh or j...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653</th>\n",
              "      <td>Hereâs a picture of grass smiling to brighte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>In his videos about self injury, not only doe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>Sometimes I feel like what happened to me isn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>I’ve been sitting on my couch playing video g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>651 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  label\n",
              "0     I've had major depressive disorder for many y...      1\n",
              "1     One of my friends lost his brother to suicide...      1\n",
              "2    My long distance boyfriend sent me flowers for...      0\n",
              "3     I was recently diagnosed with GAD (Generalize...      1\n",
              "4    After 4 years my hair is finally healed, & lif...      0\n",
              "..                                                 ...    ...\n",
              "652   I wish I could shake this. I don't laugh or j...      1\n",
              "653  Hereâs a picture of grass smiling to brighte...      0\n",
              "654   In his videos about self injury, not only doe...      1\n",
              "655   Sometimes I feel like what happened to me isn...      1\n",
              "656   I’ve been sitting on my couch playing video g...      1\n",
              "\n",
              "[651 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryFLUZKVCjhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9de31ba0-be0d-47c6-c82e-bbd6f5fa69f1"
      },
      "source": [
        "df = df.drop_duplicates()\n",
        "print(\"Banyak data pada dataset tanpa duplicate rows\", len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Banyak data pada dataset tanpa duplicate rows 651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN14IZVBDOU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af1cdaba-41b8-469a-ceff-1ad882e8c628"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "651"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWWGCE2QDQcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Duplicate dataframe\n",
        "dfNew = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVdj74hovUj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vut2xbNDxfCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "fd774022-8824-4f8d-d609-3dee2f765e66"
      },
      "source": [
        "dfNew"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've had major depressive disorder for many y...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One of my friends lost his brother to suicide...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>My long distance boyfriend sent me flowers for...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I was recently diagnosed with GAD (Generalize...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After 4 years my hair is finally healed, &amp; lif...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>I wish I could shake this. I don't laugh or j...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653</th>\n",
              "      <td>Hereâs a picture of grass smiling to brighte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>In his videos about self injury, not only doe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>Sometimes I feel like what happened to me isn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>I’ve been sitting on my couch playing video g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>651 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  label\n",
              "0     I've had major depressive disorder for many y...      1\n",
              "1     One of my friends lost his brother to suicide...      1\n",
              "2    My long distance boyfriend sent me flowers for...      0\n",
              "3     I was recently diagnosed with GAD (Generalize...      1\n",
              "4    After 4 years my hair is finally healed, & lif...      0\n",
              "..                                                 ...    ...\n",
              "652   I wish I could shake this. I don't laugh or j...      1\n",
              "653  Hereâs a picture of grass smiling to brighte...      0\n",
              "654   In his videos about self injury, not only doe...      1\n",
              "655   Sometimes I feel like what happened to me isn...      1\n",
              "656   I’ve been sitting on my couch playing video g...      1\n",
              "\n",
              "[651 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8QGCkrJ3KQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lower Casing \n",
        "def text_to_lowerCase(text):\n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eig08cgg3c3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand_contractions(data):\n",
        "  data = re.sub(r\"\\bdon't\\b\", 'do not', data)\n",
        "  data = re.sub(r\"\\bdidn't\\b\", 'did not', data)\n",
        "  data = re.sub(r\"\\bdoesn't\\b\", 'does not', data)\n",
        "  data = re.sub(r\"\\bisn't\\b\", 'is not', data)\n",
        "  data = re.sub(r\"\\baren't\\b\", 'are not', data)\n",
        "  data = re.sub(r\"\\bwasn't\\b\", 'was not', data)\n",
        "  data = re.sub(r\"\\bweren't\\b\", 'were not', data)\n",
        "\n",
        "  data = re.sub(r\"\\bhadn't\\b\", 'had not', data)\n",
        "  data = re.sub(r\"\\bhadn't've\\b\", 'had not have', data)\n",
        "  data = re.sub(r\"\\bhasn't\\b\", 'has not', data)\n",
        "  data = re.sub(r\"\\bhaven't\\b\", 'have not', data)\n",
        "\n",
        "  data = re.sub(r\"\\bcan't\\b\", 'can not', data)\n",
        "  data = re.sub(r\"\\bcan't've\\b\", 'cannot have', data)\n",
        "  data = re.sub(r\"\\bcould've\\b\", 'could have', data)\n",
        "  data = re.sub(r\"\\bcouldn't\\b\", 'could not', data)\n",
        "  data = re.sub(r\"\\bcouldn't've\\b\", 'could not have', data)\n",
        "  data = re.sub(r\"\\bshould've\\b\", 'should have', data)\n",
        "  data = re.sub(r\"\\bshouldn't\\b\", 'should not', data)\n",
        "  data = re.sub(r\"\\bshouldn't've\\b\", 'should not have', data)\n",
        "\n",
        "  data = re.sub(r\"\\bi'll\\b\", 'i will', data)\n",
        "  data = re.sub(r\"\\bi'll've\\b\", 'i will have', data)\n",
        "  data = re.sub(r\"\\bi'm\\b\", 'i am', data)\n",
        "  data = re.sub(r\"\\bi've\\b\", 'i have', data)\n",
        "  data = re.sub(r\"\\bi'd\\b\", 'i would', data)\n",
        "  data = re.sub(r\"\\bi'd've\\b\", 'i would have', data)\n",
        "\n",
        "  data = re.sub(r\"\\by'all\\b\", 'you all', data)\n",
        "  data = re.sub(r\"\\by'all're\\b\", 'you all are', data)\n",
        "  data = re.sub(r\"\\byou're\\b\", 'you are', data)\n",
        "  data = re.sub(r\"\\byou've\\b\", 'you have', data)\n",
        "  data = re.sub(r\"\\byou'll\\b\", 'you will', data)\n",
        "  data = re.sub(r\"\\byou'll've\\b\", 'you will have', data)\n",
        "  data = re.sub(r\"\\byou'd\\b\", 'you would', data)\n",
        "  data = re.sub(r\"\\byou'd've\\b\", 'you would have', data)\n",
        "\n",
        "  data = re.sub(r\"\\bwe're\\b\", 'we re', data)\n",
        "  data = re.sub(r\"\\bwe've\\b\", 'we have', data)\n",
        "  data = re.sub(r\"\\bwe'll\\b\", 'we will', data)\n",
        "  data = re.sub(r\"\\bwe'll've\\b\", 'we will have', data)\n",
        "  data = re.sub(r\"\\bwe'd\\b\", 'we would', data)\n",
        "  data = re.sub(r\"\\bwe'd've\\b\", 'we would have', data)\n",
        "\n",
        "  data = re.sub(r\"\\bthey're\\b\", 'they are', data)\n",
        "  data = re.sub(r\"\\bthey've\\b\", 'they have', data)\n",
        "  data = re.sub(r\"\\bthey'll\\b\", 'they will', data)\n",
        "  data = re.sub(r\"\\bthey'll've\\b\", 'they will have', data)\n",
        "  data = re.sub(r\"\\bthey'd\\b\", 'they would', data)\n",
        "  data = re.sub(r\"\\bthey'd've\\b\", 'they would have', data)\n",
        "\n",
        "  data = re.sub(r\"\\bhe's\\b\", 'he is', data)\n",
        "  data = re.sub(r\"\\bhe'd\\b\", 'he would', data)\n",
        "  data = re.sub(r\"\\bhe'd've\\b\", 'he would have', data)\n",
        "  data = re.sub(r\"\\bhe'll\\b\", 'he will', data)\n",
        "  data = re.sub(r\"\\bhe'll've\\b\", 'he will have', data)\n",
        "\n",
        "  data = re.sub(r\"\\bshe's\\b\", 'she is', data)\n",
        "  data = re.sub(r\"\\bshe'd\\b\", 'she would', data)\n",
        "  data = re.sub(r\"\\bshe'd've\\b\", 'she would have', data)\n",
        "  data = re.sub(r\"\\bshe'll\\b\", 'she will', data)\n",
        "  data = re.sub(r\"\\bshe'll've\\b\", 'she will have', data)\n",
        "\n",
        "  data = re.sub(r\"\\bit's\\b\", 'it is', data)\n",
        "  data = re.sub(r\"\\bit'd\\b\", 'it would', data)\n",
        "  data = re.sub(r\"\\bit'd've\\b\", 'it would have', data)\n",
        "  data = re.sub(r\"\\bit'll\\b\", 'it will', data)\n",
        "  \n",
        "  data = re.sub(r\"\\bthat's\\b\", 'that is', data)\n",
        "  data = re.sub(r\"\\bthat'd\\b\", 'that would', data)\n",
        "  data = re.sub(r\"\\bthat'd've\\b\", 'that would have', data)\n",
        "  \n",
        "  data = re.sub(r\"\\bthere's\\b\", 'there is', data)\n",
        "  data = re.sub(r\"\\bthere'd\\b\", 'there would', data)\n",
        "  data = re.sub(r\"\\bthere'd've\\b\", 'there would have', data)\n",
        "\n",
        "  data = re.sub(r\"\\bwhat's\\b\", 'what is', data)\n",
        "  data = re.sub(r\"\\bwhat're\\b\", 'what are', data)\n",
        "  data = re.sub(r\"\\bwhat'd\\b\", 'what would', data)\n",
        "  data = re.sub(r\"\\bwhat've\\b\", 'what have', data)\n",
        " \n",
        "  data = re.sub(r\"\\bwhat's\\b\", 'when is', data)\n",
        "  data = re.sub(r\"\\bwhat're\\b\", 'where is', data)\n",
        "  data = re.sub(r\"\\bwhat'd\\b\", 'who is', data)\n",
        "  data = re.sub(r\"\\bwhat've\\b\", 'who will', data)\n",
        "\n",
        "  data = re.sub(r\"\\bwill've\\b\", 'will have', data)\n",
        "  data = re.sub(r\"\\bwon't\\b\", 'will not', data)\n",
        "  data = re.sub(r\"\\bwould've\\b\", 'would have', data)\n",
        "  data = re.sub(r\"\\bwouldn't\\b\", 'would not', data)\n",
        "\n",
        "  data = re.sub(r\"\\bhow'd\\b\", 'how did', data)\n",
        "  data = re.sub(r\"\\bhow'd'yb\", 'how do you', data)\n",
        "  data = re.sub(r\"\\bhow'll\\b\", 'how will', data)\n",
        "  data = re.sub(r\"\\bhow's\\b\", 'how is', data)\n",
        "\n",
        "  data = re.sub(r\"\\bmight've\\b\", 'might have', data)\n",
        "  data = re.sub(r\"\\bmightn't\\b\", 'might not', data)\n",
        "  data = re.sub(r\"\\bmust've\\b\", 'must have', data)\n",
        "  data = re.sub(r\"\\bmustn't\\b\", 'must not', data)\n",
        "  data = re.sub(r\"\\bneedn't\\b\", 'need not', data)\n",
        "\n",
        "  data = re.sub(r\"\\b'cause\\b\", 'because', data)\n",
        "  data = re.sub(r\"\\blet's\\b\", 'let us', data)\n",
        "  data = re.sub(r\"\\bo'clock\\b\", 'of the clock', data)\n",
        "\n",
        "  data = data.split()\n",
        "  data = \" \".join(data)\n",
        "  return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXkGO-ixEE5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "    text = re.sub('\\d+', '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y_w6OWPErmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove special characters\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRTWj3-6ExXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove extra whitespaces\n",
        "def remove_extraspaces(text):\n",
        "    return re.sub(' +', ' ', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVsHwfqXEza3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenized_tweet(text):\n",
        "  return text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0nKrLUcEzhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning(text):\n",
        "  text = text_to_lowerCase(text)\n",
        "  text = expand_contractions(text)\n",
        "  text = remove_urls(text)\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_special_characters(text)\n",
        "  text = remove_extraspaces(text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi_nbw8iEzks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean data using cleaning function\n",
        "dfNew['text'] = dfNew['text'].apply(cleaning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvCn96hJFHjT",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifx_IHlrEznT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "7bf75d2e-b1d8-4fb4-ba15-bc7ba98d1447"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d8RNi4UEzqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize data\n",
        "dfTokenized = dfNew['text'].apply(tokenized_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfCwQGmUEzs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove stopwords from tokenized data\n",
        "dfTokenized = dfTokenized.apply(lambda x: [i for i in x if not i in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrZ6O6TlEzv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stem the tokenized data\n",
        "stemmer= PorterStemmer()\n",
        "dfTokenized_stemmed = dfTokenized.apply(lambda x: [stemmer.stem(i) for i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weEaXTo8Felm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatization of the tokenized data\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "dfTokenized_lemmatized = dfTokenized.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt8shyoqFgkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stitch it back together\n",
        "dfTokenized_comb = dfTokenized.apply(lambda x: ' '.join(x))\n",
        "dfTokenized_stemmed_comb = dfTokenized_stemmed.apply(lambda x: ' '.join(x))\n",
        "dfTokenized_lemmatized_comb = dfTokenized_lemmatized.apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkziiuFdFinP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dataframes for each combined tokenized\n",
        "dfTokenized_comb = pd.DataFrame(dfTokenized_comb, columns=['text', 'label'])\n",
        "dfTokenized_comb['label'] = dfNew.label\n",
        "\n",
        "dfTokenized_stemmed_comb = pd.DataFrame(dfTokenized_stemmed_comb, columns=['text', 'label'])\n",
        "dfTokenized_stemmed_comb['label'] = dfNew.label\n",
        "\n",
        "dfTokenized_lemmatized_comb = pd.DataFrame(dfTokenized_lemmatized_comb, columns=['text', 'label'])\n",
        "dfTokenized_lemmatized_comb['label'] = dfNew.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMHy_PDoFkAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b235a6e7-b3aa-4fd5-b7c8-0f742b6f36c0"
      },
      "source": [
        "len(dfTokenized_comb) == len(dfTokenized_stemmed_comb) ==  len(dfTokenized_lemmatized_comb) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pGlynvRFvj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "3acffff4-a276-46f4-b723-4416fa72a352"
      },
      "source": [
        "print(\"Check NA_Values / empty rows from dfTokenized_comb :\")\n",
        "dfTokenized_comb[dfTokenized_comb.text.isnull()==True]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check NA_Values / empty rows from dfTokenized_comb :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, label]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4qSU0IOFyjR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "21ae8468-9eea-4288-cb5b-00399c48dd2a"
      },
      "source": [
        "print(\"Check NA_Values / empty rows from dfTokenized_stemmed_comb :\")\n",
        "dfTokenized_stemmed_comb[dfTokenized_stemmed_comb.text.isnull()==True]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check NA_Values / empty rows from dfTokenized_stemmed_comb :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, label]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLkTGLewFz8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "f8c401b7-9abc-408a-8078-9d2db18b05ca"
      },
      "source": [
        "print(\"Check NA_Values / empty rows from dfTokenized_lemmatized_combs :\")\n",
        "dfTokenized_lemmatized_comb[dfTokenized_lemmatized_comb.text.isnull()==True]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check NA_Values / empty rows from dfTokenized_lemmatized_combs :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, label]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns7kXeBzF0-D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8ba4a2ac-1437-4f43-c86f-7b68bb5c1c88"
      },
      "source": [
        "# Remove NA Values from each datasets\n",
        "dfTokenized_comb = dfTokenized_comb[dfTokenized_comb.text.isnull()==False]\n",
        "print(\"Banyak data pada dataset tanpa stopword (dfTokenized_comb) :\", len(dfTokenized_comb))\n",
        "\n",
        "dfTokenized_stemmed_comb = dfTokenized_stemmed_comb[dfTokenized_stemmed_comb.text.isnull()==False]\n",
        "print(\"Banyak data pada dataset tanpa stopword yang telah distem(dfTokenized_stemmed_comb) :\", len(dfTokenized_stemmed_comb))\n",
        "\n",
        "dfTokenized_lemmatized_comb = dfTokenized_lemmatized_comb[dfTokenized_lemmatized_comb.text.isnull()==False]\n",
        "print(\"Banyak data pada dataset tanpa stopword yang telah dilemmatize (dfTokenized_lemmatized_comb) :\", len(dfTokenized_lemmatized_comb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Banyak data pada dataset tanpa stopword (dfTokenized_comb) : 651\n",
            "Banyak data pada dataset tanpa stopword yang telah distem(dfTokenized_stemmed_comb) : 651\n",
            "Banyak data pada dataset tanpa stopword yang telah dilemmatize (dfTokenized_lemmatized_comb) : 651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar45BxOiF3Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop duplicates rows from each dataset\n",
        "dfTokenized_comb = dfTokenized_comb.drop_duplicates()\n",
        "dfTokenized_stemmed_comb = dfTokenized_stemmed_comb.drop_duplicates()\n",
        "dfTokenized_lemmatized_comb = dfTokenized_lemmatized_comb.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moJECHosF4hB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3a509187-e1a7-4dc2-e831-7f218677947a"
      },
      "source": [
        "print(\"Banyak data pada dataset tanpa stopword (dfTokenized_comb) :\", len(dfTokenized_comb))\n",
        "print(\"Banyak data pada dataset tanpa stopword yang telah distem(dfTokenized_stemmed_comb) :\", len(dfTokenized_stemmed_comb))\n",
        "print(\"Banyak data pada dataset tanpa stopword yang telah dilemmatize (dfTokenized_lemmatized_comb) :\", len(dfTokenized_lemmatized_comb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Banyak data pada dataset tanpa stopword (dfTokenized_comb) : 649\n",
            "Banyak data pada dataset tanpa stopword yang telah distem(dfTokenized_stemmed_comb) : 649\n",
            "Banyak data pada dataset tanpa stopword yang telah dilemmatize (dfTokenized_lemmatized_comb) : 649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTkRE3NUF6XS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "9c2f99f1-2219-4f92-9e66-db53abbb7fba"
      },
      "source": [
        "print(\"Check NA_Values pada dfDataset : \")\n",
        "dfNew[dfNew.text.isnull() == True]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check NA_Values pada dfDataset : \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, label]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH_nlqkEF9Zx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b27ccdc4-6adb-4242-816f-412750a46ce9"
      },
      "source": [
        "# Drop duplicates from dfDataset\n",
        "dfNew = dfNew.drop_duplicates()\n",
        "print(\"Banyak ada pada dataset dengan stopword (dfDataset) : \", len(dfNew))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Banyak ada pada dataset dengan stopword (dfDataset) :  649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wzyjNYDGKSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listCombined = [dfNew, dfTokenized_comb, dfTokenized_stemmed_comb, dfTokenized_lemmatized_comb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-yVnvxlGN1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset index for each datasets\n",
        "for i in range (0,len(listCombined)):\n",
        "  listCombined[i] = listCombined[i].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzzK32KiGahv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find word_count from each dataset\n",
        "for i in range (0,len(listCombined)):\n",
        "  listCombined[i]['word_count'] = listCombined[i].text.apply(lambda x: len(str(x).split()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_sXLbHjGb07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "02a0bc44-616b-4178-aaed-797c98ec0a66"
      },
      "source": [
        "for i in range (0, len(listCombined)):\n",
        "  print(listCombined[i][listCombined[i].word_count >= 2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  text  label  word_count\n",
            "0    i have had major depressive disorder for many ...      1         132\n",
            "1    one of my friends lost his brother to suicide ...      1         110\n",
            "2    my long distance boyfriend sent me flowers for...      0          27\n",
            "3    i was recently diagnosed with gad generalized ...      1         101\n",
            "4    after years my hair is finally healed life is ...      0          57\n",
            "..                                                 ...    ...         ...\n",
            "644  i wish i could shake this i do not laugh or jo...      1          48\n",
            "645  heres a picture of grass smiling to brighten u...      0          11\n",
            "646  in his videos about self injury not only does ...      1         209\n",
            "647  sometimes i feel like what happened to me is n...      1          60\n",
            "648  ive been sitting on my couch playing video gam...      1          90\n",
            "\n",
            "[646 rows x 3 columns]\n",
            "                                                  text  label  word_count\n",
            "0    major depressive disorder many years got point...      1          56\n",
            "1    one friends lost brother suicide years ago sti...      1          48\n",
            "2    long distance boyfriend sent flowers particula...      0          12\n",
            "3    recently diagnosed gad generalized anxiety dis...      1          51\n",
            "4    years hair finally healed life also getting mu...      0          31\n",
            "..                                                 ...    ...         ...\n",
            "644  wish could shake laugh joke life voice anymore...      1          22\n",
            "645           heres picture grass smiling brighten day      0           6\n",
            "646  videos self injury almost completely ignore fo...      1         119\n",
            "647  sometimes feel like happened bad enough ptsd s...      1          30\n",
            "648  ive sitting couch playing video games sleeping...      1          43\n",
            "\n",
            "[637 rows x 3 columns]\n",
            "                                                  text  label  word_count\n",
            "0    major depress disord mani year got point somet...      1          56\n",
            "1    one friend lost brother suicid year ago still ...      1          48\n",
            "2    long distanc boyfriend sent flower particular ...      0          12\n",
            "3    recent diagnos gad gener anxieti disord hasnt ...      1          51\n",
            "4    year hair final heal life also get much better...      0          31\n",
            "..                                                 ...    ...         ...\n",
            "644  wish could shake laugh joke life voic anymor a...      1          22\n",
            "645               here pictur grass smile brighten day      0           6\n",
            "646  video self injuri almost complet ignor form se...      1         119\n",
            "647  sometim feel like happen bad enough ptsd somet...      1          30\n",
            "648  ive sit couch play video game sleep day hour h...      1          43\n",
            "\n",
            "[637 rows x 3 columns]\n",
            "                                                  text  label  word_count\n",
            "0    major depressive disorder many year got point ...      1          56\n",
            "1    one friend lost brother suicide year ago still...      1          48\n",
            "2    long distance boyfriend sent flower particular...      0          12\n",
            "3    recently diagnosed gad generalized anxiety dis...      1          51\n",
            "4    year hair finally healed life also getting muc...      0          31\n",
            "..                                                 ...    ...         ...\n",
            "644  wish could shake laugh joke life voice anymore...      1          22\n",
            "645            here picture grass smiling brighten day      0           6\n",
            "646  video self injury almost completely ignore for...      1         119\n",
            "647  sometimes feel like happened bad enough ptsd s...      1          30\n",
            "648  ive sitting couch playing video game sleeping ...      1          43\n",
            "\n",
            "[637 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtAbTAvCGdKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (0, len(listCombined)):\n",
        "  listCombined[i] = listCombined[i][listCombined[i].word_count >= 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u6vOMVTGe5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6fdc7c3f-2783-4113-ace6-3d1bcf3f2807"
      },
      "source": [
        "for item in listCombined:\n",
        "  print(len(item))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "646\n",
            "637\n",
            "637\n",
            "637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ju8s_acGf30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset index for each datasets\n",
        "for i in range (0,len(listCombined)):\n",
        "  listCombined[i] = listCombined[i].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoyfKkFqGhW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files #library untuk export dari googlecolab\n",
        "# Export dfDataset (dataset preprocessed - 1)\n",
        "listCombined[0].to_csv('dataset_preprocessed_1.csv')\n",
        "\n",
        "# Export dfTokenized (dataset preprocessed - 2)\n",
        "listCombined[1].to_csv('dataset_preprocessed_2.csv')\n",
        "\n",
        "# Export dfTokenized_stemmed (dataset preprocessed - 2)\n",
        "listCombined[2].to_csv('dataset_preprocessed_2_stem.csv')\n",
        "\n",
        "# Export dfTokenized_lemmatized (dataset preprocessed - 2)\n",
        "listCombined[3].to_csv('dataset_preprocessed_2_lemma.csv')\n",
        "\n",
        "files.download('dataset_preprocessed_1.csv')\n",
        "files.download('dataset_preprocessed_2.csv')\n",
        "files.download('dataset_preprocessed_2_stem.csv')\n",
        "files.download('dataset_preprocessed_2_lemma.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgKmatjmHCYB",
        "colab_type": "text"
      },
      "source": [
        "stem dan lemma 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyQOpwr3G--i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "213dfd00-59ce-499c-94bb-ffda6c81fa94"
      },
      "source": [
        "!wget -O dfPre1.csv https://raw.githubusercontent.com/godhi06/TA/master/data/preprocessing/dataset_preprocessed_1.csv\n",
        "\n",
        "dfPre1 = pd.read_csv('dfPre1.csv', index_col=0)\n",
        "print(\"Banyak data pada dfPre1 : \", len(dfPre1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-19 17:34:07--  https://raw.githubusercontent.com/godhi06/TA/master/data/preprocessing/dataset_preprocessed_1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 215246 (210K) [text/plain]\n",
            "Saving to: ‘dfPre1.csv’\n",
            "\n",
            "\rdfPre1.csv            0%[                    ]       0  --.-KB/s               \rdfPre1.csv          100%[===================>] 210.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-06-19 17:34:08 (4.98 MB/s) - ‘dfPre1.csv’ saved [215246/215246]\n",
            "\n",
            "Banyak data pada dfPre1 :  647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTUF_S_FIdyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize data\n",
        "dfPre1_Token = dfPre1['text'].apply(tokenized_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XBDu7IhIhmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stem the tokenized data\n",
        "stemmer = PorterStemmer()\n",
        "dfPre1_Token_stemmed = dfPre1_Token.apply(lambda x: [stemmer.stem(i) for i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1ZVWgjUIkD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatization of the tokenized data\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "dfPre1_Token_lemmatized = dfPre1_Token.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB6MI7F_Il50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "a17348ca-0abb-416f-c8ad-6b3b9f295168"
      },
      "source": [
        "dfPre1_Token_stemmed.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "227    [i, alway, thought, it, wouldnt, happen, becau...\n",
              "608    [for, our, two, year, anniversari, my, boyfrie...\n",
              "556    [despit, the, shit, show, that, our, democraci...\n",
              "346    [when, peopl, chew, loudli, or, when, they, ke...\n",
              "180    [when, i, start, to, feel, more, normal, i, wa...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlRwfuSmIneo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "ac779d68-cb68-4771-b289-d6bcb57c2ec9"
      },
      "source": [
        "dfPre1_Token_lemmatized.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604    [my, family, and, i, decided, to, postpone, th...\n",
              "15     [happy, young, man, today, finally, getting, m...\n",
              "165    [my, boyfriend, came, into, town, for, the, we...\n",
              "439    [friend, took, this, of, me, during, a, day, o...\n",
              "308    [im, going, through, a, lot, right, now, im, n...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AVerzHzIpMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stitch it back together\n",
        "dfPre1_Token_stemmed = dfPre1_Token_stemmed.apply(lambda x: ' '.join(x))\n",
        "dfPre1_Token_lemmatized = dfPre1_Token_lemmatized.apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbHQDThJIr4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy dfPre1 to get the label\n",
        "dfPre1_stemmed = dfPre1.copy()\n",
        "dfPre1_lemma = dfPre1.copy()\n",
        "\n",
        "# Change each dfPre1 with the respective token variabel\n",
        "dfPre1_stemmed.text = dfPre1_Token_stemmed\n",
        "dfPre1_lemma.text = dfPre1_Token_lemmatized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRfTIBHUIu1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "0f0c4a55-7fe9-443d-854b-73f8861e932f"
      },
      "source": [
        "# Cek tiap text pada dfPre1\n",
        "print(\"Lihat 5 teks random dari dfPre1_stemmed :\")\n",
        "for item in dfPre1_stemmed.text.sample(5):\n",
        "  print(item)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Lihat 5 teks random dari dfPre1_lemma :\")\n",
        "for item in dfPre1_lemma.text.sample(5):\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lihat 5 teks random dari dfPre1_stemmed :\n",
            "whenev i appear around her she suddenli ha to leav and doe so within minut of me be there it is been happen sinc our last long talk that talk wa over a month ago and i thought she would be over what i said by now in my opinion it realli wa not that bad and ani feel i have toward her have been buri deep becaus she is now in a relationship the relationship doe not bother me at all but i feel like what i said to her did and i am complet confus becaus she ask me to say it howev i feel like a complet moron for do so and i regret it more than i have regret ani other move i have made by mistak deep friendship for love\n",
            "my daili routin late that ha work wonder also been repeat thi mantra as i have sever anxieti dont worri about all of those what if thing arent a problem until theyr a problem the anxieti over what you might have done wrong or might do wrong or not actual problem that exist yet if they becom a problem youll handl and solv them it okay your all good and your capabl of whatev life throw at you\n",
            "firefight celebr as torrenti rain in eastern australia extinguish a third of the fire in the region\n",
            "i feel so raw i feel like a walk wound i never stop hurt and the slightest emot brush open everyth back up my emot are so strong they will kill me one day if i let them everyth hurt me i am too sensit overli sensit i get overwhelm at almost noth\n",
            "thank to emdr therapi i feel someth i havent felt in almost year hope for my futur\n",
            "\n",
            "\n",
            "Lihat 5 teks random dari dfPre1_lemma :\n",
            "i have had fibromyalgia and chronic fatigue for a few year now have not gone to school in six year and i have been homeschooled in the meantime but it is been hard taking everything in due to brain fog just went to my first adult ed class chilled atmosphere and nice people first time having conversation with people that are not family member in a while so that wa awesome altogeth i am feeling happier just to get out and see people and to actually learn thing easily for the first time in a while\n",
            "i feel so alone and worthless i cant\n",
            "but i should not feel happy i am currently sitting doing nothing i am sitting the biggest set of exam i have had to face yet tomorrow and to say i am illprepared is an understatement but the biggest wave of happiness and excitement and joy ha rushed over me can anyone explain ha anyone felt like this am i crazy\n",
            "i am and i have not cut in month my scar are starting to fade and it make me want to make new one if the scar fade it is like it never really happened what do i do\n",
            "worked several year for a company that wa started in the late s there are only a few of u left but the bos still buy u doughnut im grateful and planning to stay until the wheel fall off\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhRm_LgyIwdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files #library untuk export dari googlecolab\n",
        "\n",
        "# Export dfTokenized_stemmed (dataset preprocessed - 1_stemmed)\n",
        "dfPre1_stemmed.to_csv('dataset_preprocessed_1_stemmed.csv')\n",
        "\n",
        "# Export dfTokenized_lemmatized (dataset preprocessed - 1_lemma)\n",
        "dfPre1_lemma.to_csv('dataset_preprocessed_1_lemma.csv')\n",
        "\n",
        "files.download('dataset_preprocessed_1_stemmed.csv')\n",
        "files.download('dataset_preprocessed_1_lemma.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5CM2thQIyuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}